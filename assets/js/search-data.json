{
  
    
        "post0": {
            "title": "Feature engineering",
            "content": "Definition . Feature engineering is the process of transforming raw data into features that better represent the underlying signal to be fed to a machine learning model, resulting in improved model accuracy on unseen data. -- Derek Jedamski . Motivation . Feature engineering is the most underrated topic in the machine learning field. Very few books actually concentrate on feature engineering techniques. The success of any machine learning model is depending on input features. . Garbage In, Garbage Out (GIGO) : The quality of the output is highly dependent on the quality of the input. If you feed the bad features to the model, you will not get high quality output. Even though you have a lot of data and an incredible algorithm but with poorly defined features your model will perform poorly. . Advantages of feature engineering: . Model flexibility: With nice features, the choice of algorithm and hyperparameter tuning become less important. | Simpler models: with nice features, we don&#39;t need super complex algorithm. Even the simpler model can provide better result. These days, complex models are getting more attention, but simpler models are easy to understand, easy to debug, easy to optimize and often run faster than complex models. In the real world, if our model runs a couple of seconds faster, it is a huge win. | Better Result: Garbage in, garbage out. | . Loading data . We will use famous Titanic dataset. We will use pandas to read csv file. . titanic = pd.read_csv(&#39;data/titanic.csv&#39;) titanic.head() . . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . Variable Defination Key . Survived | Survival | 0 = No, 1 = Yes | . Pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd | . Sex | Sex | | . Age | Age in years | | . SibSp | # of siblings / spouses aboard the Titanic | | . Parch | # of parents / children aboard the Titanic | | . Ticket | Ticket number | | . Fare | Passenger fare | | . Cabin | Cabin number | | . Embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton | . print(f&quot;Titanic dataset has {titanic.shape[0]} entries and {titanic.shape[1]} features.&quot;) . . Titanic dataset has 891 entries and 12 features. . Data Exploration . Continuous features . If we drop all categorical variables, it will look like below (Categirocal features are important but for now we will explore continuous features only): . cat_feat = [&#39;PassengerId&#39;, &#39;Name&#39;, &#39;Ticket&#39;, &#39;Sex&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;] titanic.drop(cat_feat, axis=1, inplace=True) titanic.head() . . Survived Pclass Age SibSp Parch Fare . 0 0 | 3 | 22.0 | 1 | 0 | 7.2500 | . 1 1 | 1 | 38.0 | 1 | 0 | 71.2833 | . 2 1 | 3 | 26.0 | 0 | 0 | 7.9250 | . 3 1 | 1 | 35.0 | 1 | 0 | 53.1000 | . 4 0 | 3 | 35.0 | 0 | 0 | 8.0500 | . titanic.describe() . . Survived Pclass Age SibSp Parch Fare . count 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | . std 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | . min 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | . 25% 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | . 50% 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | . max 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | . The table above reveals that: . Target variable Survived is binary with mean 0.38 which means 38% of the people survived. The dataset is not highly imbalanced. In the case of highly imbalanced dataset one should use oversampling or downsampling technique. | Pclass, SibSp and Parch of descreate nature and have limited values. | Titanic dataset has 819 entries but for age it shows only 714 entries. It means Age feature has 177 missing values . | . Now, let&#39;s explore correlation metrics. Keep in mind, negative correlation is equally important as positive correlation. The features used to predict the target variable should be less correlated with each other because it can confuse the model as it cannot parse out from which feature the signal is coming from. . print(&quot;Correlation matrix ⬇️&quot;) titanic.corr() . . Correlation matrix ⬇️ . Survived Pclass Age SibSp Parch Fare . Survived 1.000000 | -0.338481 | -0.077221 | -0.035322 | 0.081629 | 0.257307 | . Pclass -0.338481 | 1.000000 | -0.369226 | 0.083081 | 0.018443 | -0.549500 | . Age -0.077221 | -0.369226 | 1.000000 | -0.308247 | -0.189119 | 0.096067 | . SibSp -0.035322 | 0.083081 | -0.308247 | 1.000000 | 0.414838 | 0.159651 | . Parch 0.081629 | 0.018443 | -0.189119 | 0.414838 | 1.000000 | 0.216225 | . Fare 0.257307 | -0.549500 | 0.096067 | 0.159651 | 0.216225 | 1.000000 | . The target variable Survived has positive correlation with the feature Fare. Passengers who bought costly ticket had higher chance of survival. . Fare and Pclass have negative correlation. It means ticket class 1 is the costliest and 3 is the cheapest. . titanic.groupby(&#39;Pclass&#39;)[&#39;Fare&#39;].describe() . . count mean std min 25% 50% 75% max . Pclass . 1 216.0 | 84.154687 | 78.380373 | 0.0 | 30.92395 | 60.2875 | 93.5 | 512.3292 | . 2 184.0 | 20.662183 | 13.417399 | 0.0 | 13.00000 | 14.2500 | 26.0 | 73.5000 | . 3 491.0 | 13.675550 | 11.778142 | 0.0 | 7.75000 | 8.0500 | 15.5 | 69.5500 | . 75 percentile of Pclass-1 &gt; 75 percentile of Pclass-2 75 percentile of Pclass-3 . def describe_count_feature(feature): print(f&quot; n****** Result for {feature} ******&quot;) print(titanic.groupby(&#39;Survived&#39;)[feature].describe() ) ttest(feature) def ttest(feature): survived_feature = titanic[titanic.Survived == 1] [feature] not_survived_feature = titanic[titanic.Survived == 0] [feature] tstat, pval = stats.ttest_ind(survived_feature, not_survived_feature, equal_var=False) print(f&quot;t-statistic: {tstat:.1f}, P-Vale: {pval:.3f}&quot;) for feature in titanic.columns.drop(&#39;Survived&#39;): describe_count_feature(feature) . . ****** Result for Pclass ****** count mean std min 25% 50% 75% max Survived 0 549.0 2.531876 0.735805 1.0 2.0 3.0 3.0 3.0 1 342.0 1.950292 0.863321 1.0 1.0 2.0 3.0 3.0 t-statistic: -10.3, P-Vale: 0.000 ****** Result for Age ****** count mean std min 25% 50% 75% max Survived 0 424.0 30.626179 14.172110 1.00 21.0 28.0 39.0 74.0 1 290.0 28.343690 14.950952 0.42 19.0 28.0 36.0 80.0 t-statistic: nan, P-Vale: nan ****** Result for SibSp ****** count mean std min 25% 50% 75% max Survived 0 549.0 0.553734 1.288399 0.0 0.0 0.0 1.0 8.0 1 342.0 0.473684 0.708688 0.0 0.0 0.0 1.0 4.0 t-statistic: -1.2, P-Vale: 0.233 ****** Result for Parch ****** count mean std min 25% 50% 75% max Survived 0 549.0 0.329690 0.823166 0.0 0.0 0.0 0.0 6.0 1 342.0 0.464912 0.771712 0.0 0.0 0.0 1.0 5.0 t-statistic: 2.5, P-Vale: 0.013 ****** Result for Fare ****** count mean std min 25% 50% 75% max Survived 0 549.0 22.117887 31.388207 0.0 7.8542 10.5 26.0 263.0000 1 342.0 48.395408 66.596998 0.0 12.4750 26.0 57.0 512.3292 t-statistic: 6.8, P-Vale: 0.000 . Interestingly, result for Fare shows that average ticket cost of the survived passengers is higher than the passengers who did not survive. Similar difference can be observed for mean, median and inter quantile ranges. . Average age of the survived passenger ➡️ 30.63 years Average age of the non-survived passenger ➡️ 28.34 years However, 50 percentile for survived and not survived is the same. . We saw above that Age has 177 missing value. It is important to understand if the age is missing for certain groups of people OR if it is missing in a systematic fashion. This will decide how we will handle the missing value. . print(titanic.groupby(titanic.Age.isnull()).mean()) print(&#39;--&#39;) print(&quot;⬆️ True indicates missing value.&quot;) . . Survived Pclass Age SibSp Parch Fare Age False 0.406162 2.236695 29.699118 0.512605 0.431373 34.694514 True 0.293785 2.598870 NaN 0.564972 0.180791 22.158567 -- ⬆️ True indicates missing value. . People without age reported are less likely to be survived, fewer parents/children, slightly higher class number and less fare. . Visualize continuous features . Next, Let&#39;s plot the data to understand the distribution and the target variable. . fig, axs = plt.subplots(ncols=2, figsize=(16,9)) axs_no = 0 for feature in [&#39;Age&#39;, &#39;Fare&#39;]: died = list(titanic[titanic.Survived == 0][feature].dropna()) survived = list(titanic[titanic.Survived == 1][feature].dropna()) xmin = min(min(died), min(survived)) xmax = max( max(died), max(survived) ) width = (xmax - xmin) / 40 sns.histplot(died, color=&#39;r&#39;, kde=False, bins=np.arange(xmin, xmax, width), ax=axs[axs_no]) sns.histplot(survived, color=&#39;g&#39;, kde=False, bins=np.arange(xmin, xmax, width), ax=axs[axs_no]) axs[axs_no].set_title(f&quot;Figure 1.{axs_no+1}: Histogram for {feature}&quot;) axs[axs_no].legend([&#39;Did not survived&#39;, &#39;Survived&#39;]) axs_no += 1 . . In the section above, we observed almost no difference in the age for the group who survived and who did not. The histogram shown in figure 1.1 confirms that. . Previously, we observed that on average, passengers who survived paid a very high price for the ticket as compared to passengers who did not survive. . Mean Fare for people who survived ➡️ ~48 | Mean Fare for people who did not survive ➡️ ~22 | . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The overlaid histogram (Figure 1.2) highlights the caution you have to take by looking only at averages instead of the full distribution. Except the first bin of the histogram, the likelihood of surviving versus not surviving is very similar. For example, in the case of the second bin, we can observe $ approx$ 105 for people who did not survive and $ approx$ 70 for people who survived.  . The histogram for feature Fare (Figure 1.2) shows minor differences between the two distributions, unlike what we observed by comparing mean values. The big difference between mean values is influenced by some outliers. . fig, axs = plt.subplots(ncols=3, figsize=(30,9)) axs_no = 0 ordinal_features = [&#39;Pclass&#39;, &#39;SibSp&#39;, &#39;Parch&#39;] for feature in ordinal_features: axs[axs_no].tick_params(axis=&#39;x&#39;, labelsize=13) axs[axs_no].tick_params(axis=&#39;y&#39;, labelsize=13) axs[axs_no].set_xlabel(f&quot;{feature}&quot;, fontsize=15) axs[axs_no].set_ylabel(&quot;Survived&quot;, fontsize=15) sns.pointplot(x=feature, y=&#39;Survived&#39;, data=titanic, ax=axs[axs_no]) axs[axs_no].set_title(f&quot;Figure 2.{axs_no+1}: Input feature --&gt; &#39;{feature}&#39; &quot;, fontsize=20) axs_no += 1 plt.ylim(0,1) . . Figure 2.x shows the percentage of people survived at each level of the input feature. The vertical lines represent the confidence level. Input with limited examples will result in a bigger line and represent less confidence. . Figure 2.1 depicts that first class passengers (Pclass=1) are the most likely to survive (around 64% probability) and third class passengers are the least likely to survive. Here, we can see a clear trend. . Likelihood of first class passenger survive &gt; Likelihood of second class passenger survive &gt; Likelihood of third class passenger survive . Figure 2.2 and figure 2.3 show that as the number of family members increases, the likelihood of surviving is reduced. Both plots show almost similar trends, therefore we can combine these two features and reduce redundancy. We have combined the features SibSp and Parch and added a new feature called family_members. . family members = (number of siblings / spouses aboard the Titanic) + (number of parents / children aboard the Titanic) . $family _member = SibSp + Parch$ . We will decide later whether we should use the reduced feature (family_members) OR use two separate features. . titanic[&#39;family_members&#39;] = titanic[&#39;SibSp&#39;] + titanic[&#39;Parch&#39;] sns.pointplot(x=&#39;family_members&#39;, y=&#39;Survived&#39;, data=titanic) plt.title(f&quot;Figure 3: Total family members (SibSp + Parch)&quot;) plt.show() . . We can observe from figure 3 that the probability of surviving increases till three family_members and after that we are observing sudden drop in the values. . Categirocal features . Now, let&#39;s explore the categorical features. . titanic = pd.read_csv(&#39;data/titanic.csv&#39;) continuous_features = [&#39;Pclass&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Age&#39;, &#39;PassengerId&#39;] titanic.drop(columns=continuous_features, axis=1, inplace=True) categorical_features = list(titanic.columns) titanic.head() . . Survived Name Sex Ticket Cabin Embarked . 0 0 | Braund, Mr. Owen Harris | male | A/5 21171 | NaN | S | . 1 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | PC 17599 | C85 | C | . 2 1 | Heikkinen, Miss. Laina | female | STON/O2. 3101282 | NaN | S | . 3 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 113803 | C123 | S | . 4 0 | Allen, Mr. William Henry | male | 373450 | NaN | S | . print(&quot;⬇️ Total NaN values for each categorical feature.&quot;) print(titanic.isnull().sum()) . . ⬇️ Total NaN values for each categorical feature. Survived 0 Name 0 Sex 0 Ticket 0 Cabin 687 Embarked 2 dtype: int64 . Now, we will see the number of unique value for each categorical feature. . number_of_unique_values = dict() for feature in titanic.drop(&#39;Survived&#39;, axis=1).columns: number_of_unique_values[feature] = titanic[feature].nunique() data = np.array([list(number_of_unique_values.keys()), list(number_of_unique_values.values())]) pd.DataFrame(data=data.T, columns=[&#39;features&#39;, &#39;# of unique values&#39;]) . . features # of unique values . 0 Name | 891 | . 1 Sex | 2 | . 2 Ticket | 681 | . 3 Cabin | 147 | . 4 Embarked | 3 | . For the Cabin feature, we have 687 NaN values (out of 891 entries). Out of 204 available values, 147 are unique. At first glance, this feature does not look useful for the prediction. Let&#39;s further investigate this feature for better understanding. . print(titanic.groupby(titanic.Cabin.isnull()).mean()) print(&quot;⬆️ True indicates the missing value.&quot;) . . Survived Cabin False 0.666667 True 0.299854 ⬆️ True indicates the missing value. . The code cell above reveals that only $ approx$ 30% passengers without Cabin information could servive. What could have happened that actually no cabin was allocated to few passengers and they were sleeping at one place. Here, reason is not important. We can replace this feature with Cabin_info feature which indicates that whether the Cabin information is available or not. Cabin_info can be seen as a feature with binary values. . titanic[&#39;Cabin_info&#39;] = np.where(titanic.Cabin.isnull(), 0, 1) . The titanic dataset has 681 unique ticket values out of 891 values with 0 NaN value. From this feature we will not get any useful signal and therefore we will drop the Ticket column. Readers can explore this feature further (small exercise if you are interested). . Next, let&#39;s investigate the feature Name. Logically, name cannot have any influence whether he survived or not. However, if the passenger has a special title (like Master, prof. etc.), it can give him special privileges. It is worth exploring. . print(&quot;5 most common titles ⬇️&quot;) titanic[&#39;Title_raw&#39;] = titanic.Name.apply(lambda x: x.split(&#39;,&#39;)[1].split(&#39;.&#39;)[0].strip()) titanic.pivot_table(&#39;Survived&#39;, index=[&#39;Title_raw&#39;, &#39;Sex&#39;], aggfunc=[&#39;count&#39;, &#39;mean&#39;]).sort_values([( &#39;count&#39;, &#39;Survived&#39;)], ascending=False) [:5] . . 5 most common titles ⬇️ . count mean . Survived Survived . Title_raw Sex . Mr male 517 | 0.156673 | . Miss female 182 | 0.697802 | . Mrs female 125 | 0.792000 | . Master male 40 | 0.575000 | . Rev male 6 | 0.000000 | . The pivot table above reveals that a name with the title &quot;Mrs&quot; has $ approx$ 80% chance of survival, name with the title &quot;Miss&quot; has $ approx$ 70% of chance of survival. We will keep the 4 most common titles and group the others (please refer to feature Title in the dataframe below). . titanic[&#39;Title&#39;] = titanic.Title_raw.apply(lambda x: x if x in [&#39;Mr&#39;, &#39;Miss&#39;, &#39;Mrs&#39;, &#39;Master&#39;] else &#39;other&#39;) titanic.drop([&#39;Name&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;Title_raw&#39;],axis=1).sample(frac=1).reset_index(drop=True). head(8) . . Survived Sex Embarked Cabin_info Title . 0 1 | male | C | 1 | Mr | . 1 1 | female | S | 1 | Mrs | . 2 1 | male | S | 0 | Mr | . 3 0 | male | C | 0 | other | . 4 0 | male | S | 1 | Mr | . 5 0 | male | S | 0 | Mr | . 6 0 | male | C | 0 | Mr | . 7 1 | male | S | 1 | Mr | . fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(40,19)) axs_no = 0 n_row = 0 n_col = 0 for feature in [&#39;Title&#39;, &#39;Sex&#39;, &#39;Cabin_info&#39;, &#39;Embarked&#39;]: axs[n_row][n_col].tick_params(axis=&#39;x&#39;, labelsize=15) axs[n_row][n_col].tick_params(axis=&#39;y&#39;, labelsize=15) sns.pointplot(x=feature, y=&#39;Survived&#39;, data=titanic, ax=axs[n_row][n_col]) axs[n_row][n_col].set_title(f&quot;Figure 4.{axs_no}: {feature}&quot;, fontsize=25) axs[n_row][n_col].set_xlabel(f&#39;{feature}&#39;, fontsize=23) axs[n_row][n_col].set_ylabel(&#39;Survived&#39;, fontsize=23) axs_no += 1 n_row = 1 if axs_no &gt; 1 else 0 n_col = 1 if n_col == 0 else 0 . . The likelihood of survival cannot be dependent on the embarkation port. Passengers from one port can be richer than other ports and, therefore, they can afford a ticket with cabin (Pivot table below confirms our hypothesis). However, we are considering Cabin_info separately, and therefore we can ignore the Embarked feature. We will drop the feature shown in figure 4.3. . titanic.pivot_table(&#39;Survived&#39;, index=[&#39;Cabin_info&#39;], columns=[&#39;Embarked&#39;], aggfunc=&#39;count&#39;) . . Embarked C Q S . Cabin_info . 0 99 | 73 | 515 | . 1 69 | 4 | 129 | . Summary . Name &nbsp; : Name on its own was not very valuable. Somebody&#39;s name probably didn&#39;t determine whether they are likely to survive. However, name title can be the proxy for a status and is likely related to whether they survive or not. Therefore, we have decided Title is more useful feature than Name. | Pclass &nbsp; : Likelihood of first class passenger survive &gt; Likelihood of second class passenger survive &gt; Likelihood of third class passenger survive (as shown in figure 2.1). | Sex &nbsp;: Female has $ approx$ 74% chance of survival and male has only $ approx$ 19% chance of survival (as shown in figure 4.1). | Age &nbsp;: As shown in figure 1.1 Age distribution for both cases (survived and not survived) are the same. | SibSp + Parch &nbsp;: We realized that these two features tells the same story and therefore we have decided to combine those into one feature. | Ticket &nbsp;: We validated that ticket number is random and does not provide any important information. | Fare &nbsp;: It is more correlated to Pclass. In the future we can investigate if we can use only one of these features to achieve the same or better accuracy. | Cabin &nbsp;: Cabin is missing for more than 75% of passengers. We could have assumed that it was missing randomly and dropped the entire feature. However, we uncovered the fact that there was a strong correlation between the cabin being missing and the survival rate. Therefore, we converted this categorical feature to a binary indicator that seems to be very powerful predictor of whether a passenger survived. | Embarked &nbsp;: We have concluded that it is not a causal factor. It is likely correlated with some other feature and that other feature is probably the driving factor here. | . Creating and cleaning features . Handling missing values in the data . Null values . PassengerId 0 | . Survived 0 | . Pclass 0 | . Name 0 | . Sex 0 | . Age 177 | . SibSp 0 | . Parch 0 | . Ticket 0 | . Fare 0 | . Cabin 687 | . Embarked 2 | . The table above shows the missing values for each input feature. Age has 177 missing values, Cabin has 687 missing values, and Embarked has only two missing values. It is very important to investigate data to come up with good imputation strategy. . Age . In the Feature exploration section, we saw that the age distribution for passengers who survived and who did not is the same. Therefore, we will assume that it is missing randomly and replace it with mean value. This is the most naive but useful method. This way, it satisfies the model by making sure we have value in there, but by replacing it with average, it is not biasing the model towards one outcome or another. . Embarked . Embarked has only two unknown values. We can replace the missing value with the most frequent value. . Cabin . In the previous section, we saw that only $ approx$ 30% passengers without cabin information could survive. What could have happened that actually no cabin was allocated to few passengers and they were sleeping at one place. Here, reason is not important. We can replace this feature with Cabin_info feature which indicates that whether the Cabin information is available or not. Cabin_info can be seen as a feature with binary values. . titanic[&#39;Age_clean&#39;] = titanic[&#39;Age&#39;].fillna(titanic.Age.mean()) titanic[&#39;Embarked_clean&#39;] = titanic[&#39;Embarked&#39;].fillna(&#39;S&#39;) titanic[&#39;Cabin_info&#39;] = np.where(titanic.Cabin.isnull(), 0, 1) . Removing outliers . In order to detact outliers we have to consider it&#39;s distibution. we can detect outliers for Fare for each Pclass and clip the value accordingly. For now we will remove outliers in 1-dimensional space and consider each feature as whole. . maximum value for features Fare and age look little extreme (in the table below compare max value with 75%) but rest seems okay. . titanic.describe() . . PassengerId Survived Pclass Age SibSp Parch Fare Age_clean Cabin_info . count 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | 29.699118 | 0.228956 | . std 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | 13.002015 | 0.420397 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | 0.420000 | 0.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | 22.000000 | 0.000000 | . 50% 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | 29.699118 | 0.000000 | . 75% 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | 35.000000 | 0.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | 80.000000 | 1.000000 | . def detect_outliers(feature): outliers = [] data = titanic[feature] mean = np.mean(data) std = np.std(data) for entry in data: zscore = (entry - mean) / std if zscore &gt; 3: outliers.append(zscore) print(f&quot; nOutliers for {feature}&quot;) print(f&quot; -- 95p: {data.quantile(0.95)} / Exceed values: {sum(data &gt; data.quantile(0.95)) }&quot;) print(f&quot; -- 3sd: {mean + (3*std)} / Exceed values: {len(outliers) }&quot;) print(f&quot; -- 99p: {data.quantile(0.99)} / Exceed values: {sum(data &gt; data.quantile(0.99)) }&quot;) for feature in [&#39;Fare&#39;, &#39;Age_clean&#39;]: detect_outliers(feature) . . Outliers for Fare -- 95p: 112.07915 / Exceed values: 45 -- 3sd: 181.20081130289697 / Exceed values: 20 -- 99p: 249.00622000000035 / Exceed values: 9 Outliers for Age_clean -- 95p: 54.0 / Exceed values: 42 -- 3sd: 68.68326826542591 / Exceed values: 7 -- 99p: 65.0 / Exceed values: 8 . 9 values exceed the 0.99 percentile for feature Fare and 8 values for feature Age. . titanic[&#39;Fare_clean&#39;]= titanic[&#39;Fare&#39;].clip(upper=titanic[&#39;Fare&#39;].quantile(0.99)) titanic[&#39;Age_clean&#39;].clip(upper=titanic[&#39;Age_clean&#39;].quantile(0.99), inplace=True) . Transforming skewed features . Next, we will see if some transformation is necessary for each continuous feature. . fig, axs = plt.subplots(ncols=2, figsize=(15,4)) axs_no = 0 for feature in [&#39;Fare_clean&#39;, &#39;Age_clean&#39;]: data = titanic[feature] sns.histplot(data, ax=axs[axs_no], kde=True ) axs[axs_no].set_title(f&quot;Figure 6.{axs_no+1}: value distribution for {feature}&quot;) axs_no += 1 . . Figure 6.2 represents the value distribution for feature Age_clean. The distribution looks good. We have replaced missing value with the mean which results into long bar at the middle. . Figure 6.1 represents the value distribution for feature Fare_clean where we have clipped the upper value to 0.99 quantile. Clipping the values helped as before it was till 500 but still the tail of the distribution is still long. Let&#39;s try different transformations to reduce the tail and make the distribution more compact. . Now, we will investigate which Box-Cox power transformation can be use. Below you can see different trasnformation plots. (you can see plot for different transformation under show output) button. . Note: Ideally, blue dots should be align with red line. . for i in [0.5, 1, 2,3,4,5,6,7,8,9]: data_t = titanic.Fare_clean ** (1/i) qqplot(data_t, line=&#39;s&#39;) plt.title(f&quot;Transformation for 1/{i}&quot;) . . for i in [3,4,5,6,7]: if i != 0: data_t = titanic.Fare_clean ** (1/i) else: data_t = np.log(titanic.Fare_clean) n, bins, patches = plt.hist(data_t, 50, density=True) mu = np.mean(data_t) sigma = np.std(data_t) plt.plot(bins, stats.norm.pdf(bins, mu, sigma)) plt.title(f&quot;Transformation for 1/{i}&quot;) plt.show() . . All plots above looks better than initial value distribution. We will perform 1/5 transformation. . titanic[&#39;Fare_clean_tr&#39;] = titanic[&#39;Fare_clean&#39;].apply(lambda x: x ** (1/5)) titanic.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Age_clean Embarked_clean Cabin_info Fare_clean Fare_clean_tr . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 22.0 | S | 0 | 7.2500 | 1.486167 | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 38.0 | C | 1 | 71.2833 | 2.347457 | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 26.0 | S | 0 | 7.9250 | 1.512864 | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 35.0 | S | 1 | 53.1000 | 2.213191 | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 35.0 | S | 0 | 8.0500 | 1.517606 | . In data exploration we have discussed the importance of the passenger&#39;s title, total family members (SibSp+Parch), and cabin indicator. . titanic[&#39;Title&#39;] = titanic[&#39;Name&#39;].apply(lambda x: x.split(&#39;,&#39;)[1].split(&#39;.&#39;)[0].strip()) titanic[&#39;Ttile_clean&#39;] = titanic.Title.apply(lambda x: x if x in [&#39;Mr&#39;, &#39;Miss&#39;, &#39;Mrs&#39;, &#39;Master&#39;] else &#39;other&#39;) titanic[&#39;family_members&#39;] = titanic[&#39;SibSp&#39;] + titanic[&#39;Parch&#39;] titanic[&#39;Cabin_info&#39;] = np.where(titanic.Cabin.isnull(), 0, 1) titanic.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Age_clean Embarked_clean Cabin_info Fare_clean Fare_clean_tr Title Ttile_clean family_members . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 22.0 | S | 0 | 7.2500 | 1.486167 | Mr | Mr | 1 | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 38.0 | C | 1 | 71.2833 | 2.347457 | Mrs | Mrs | 1 | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 26.0 | S | 0 | 7.9250 | 1.512864 | Miss | Miss | 0 | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 35.0 | S | 1 | 53.1000 | 2.213191 | Mrs | Mrs | 1 | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 35.0 | S | 0 | 8.0500 | 1.517606 | Mr | Mr | 0 | . convert categorical feature to numeric . It is not necessary to convert categorical features to numeric features. It depends on which algorithm you choose to predict the target feature. For example, if you want to use neural networks, you have to convert those to numeric. On the other hand, decision tree or random forest can use the categorical features to predict the target. . . Note: We will use random forest model to predict target feature. However, let&#8217;s see how you can convert categorical feature to numeric. . from sklearn.preprocessing import LabelEncoder for feature in categorical_features: label_encoder = LabelEncoder() titanic[f&quot;{feature}_numeric&quot;] = label_encoder.fit_transform(titanic[feature].astype(str)) print(f&quot; n{feature}:&quot;) for i, cl in enumerate(list(label_encoder.classes_)): print(f&quot; number {i} --&gt; {cl}&quot;) print(&#39;-&#39;) titanic[[&#39;Sex&#39;, &#39;Embarked_clean&#39;, &#39;Ttile_clean&#39;, &#39;Sex_numeric&#39;, &#39;Embarked_clean_numeric&#39;, &#39;Ttile_clean_numeric&#39;]].head() . . Sex: number 0 --&gt; female number 1 --&gt; male - Embarked_clean: number 0 --&gt; C number 1 --&gt; Q number 2 --&gt; S - Ttile_clean: number 0 --&gt; Master number 1 --&gt; Miss number 2 --&gt; Mr number 3 --&gt; Mrs number 4 --&gt; other - . Sex Embarked_clean Ttile_clean Sex_numeric Embarked_clean_numeric Ttile_clean_numeric . 0 male | S | Mr | 1 | 2 | 2 | . 1 female | C | Mrs | 0 | 0 | 3 | . 2 female | S | Miss | 0 | 2 | 1 | . 3 female | S | Mrs | 0 | 2 | 3 | . 4 male | S | Mr | 1 | 2 | 2 | . Prepare features for modeling . train-test split . Next, we will do train-test split using sklearn library. We will split the dataset into two sets. The model will trained using the 60% of the data, validate the model using 20% of the data and test using another 20% of the data. We do this because our model should perform well on unseen data. I am not going into detail. There are many awesome blogs on this topic. . . Note: Our dataset is not balanced and therefore we have to consider this while splitting the data. We have set Survived feature to stratify. . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(titanic.drop(&#39;Survived&#39;, axis=1), titanic[&#39;Survived&#39;], stratify=titanic.Survived, test_size=0.4, shuffle=True) X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, stratify=y_test, test_size=0.5, shuffle=True) . print(f&quot;X_train t: {round(len(X_train)/len(titanic),2)*100}% of original dataset.&quot;) print(f&quot;X_val t: {round(len(X_val)/len(titanic),2)*100}% of original dataset.&quot;) print(f&quot;X_test t: {round(len(X_test)/len(titanic),2)*100}% of original dataset.&quot;) . . X_train : 60.0% of original dataset. X_val : 20.0% of original dataset. X_test : 20.0% of original dataset. . standardieze features (z-score) . The titanic dataset has features with different value range. For example, compare the value difference between features Age and Fare (refer the table below). Deep learning algorithm suffer from this kind features, specially logistic regression. . titanic[[&#39;Age_clean&#39;, &#39;Fare_clean_tr&#39;, &#39;SibSp&#39;, &#39;Parch&#39;]].describe() . . Age_clean Fare_clean_tr SibSp Parch . count 891.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 29.640195 | 1.802562 | 0.523008 | 0.381594 | . std 12.820616 | 0.434645 | 1.102743 | 0.806057 | . min 0.420000 | 0.000000 | 0.000000 | 0.000000 | . 25% 22.000000 | 1.512306 | 0.000000 | 0.000000 | . 50% 29.699118 | 1.706078 | 0.000000 | 0.000000 | . 75% 35.000000 | 1.987341 | 1.000000 | 0.000000 | . max 65.000000 | 3.014686 | 8.000000 | 6.000000 | . The table below represents the standardized dataset. Here, you can see that all features have mean 0 and standard deviation 1. This trick will help neural network to reach global minima faster. I am going to explain this into detail but if you need further explanation please write me. . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train_temp = pd.DataFrame() X_val_temp = pd.DataFrame() X_test_temp = pd.DataFrame() features = [&#39;Age_clean&#39;, &#39;Fare_clean_tr&#39;, &#39;Sex_numeric&#39;, &#39;SibSp&#39;, &#39;Parch&#39;] sc.fit(X_train[features]) X_train_temp[features] = sc.transform(X_train[features]) X_val_temp[features] = sc.transform(X_val[features]) X_test_temp[features] = sc.transform(X_test[features]) X_train_temp.describe() . . Age_clean Fare_clean_tr Sex_numeric SibSp Parch . count 5.340000e+02 | 5.340000e+02 | 5.340000e+02 | 5.340000e+02 | 5.340000e+02 | . mean -1.629990e-16 | -8.482603e-17 | -6.985673e-17 | -2.661209e-17 | 7.318324e-17 | . std 1.000938e+00 | 1.000938e+00 | 1.000938e+00 | 1.000938e+00 | 1.000938e+00 | . min -2.317022e+00 | -4.257209e+00 | -1.345558e+00 | -4.644797e-01 | -4.482878e-01 | . 25% -5.308145e-01 | -6.615538e-01 | -1.345558e+00 | -4.644797e-01 | -4.482878e-01 | . 50% 5.057178e-03 | -2.023385e-01 | 7.431861e-01 | -4.644797e-01 | -4.482878e-01 | . 75% 4.290821e-01 | 4.553313e-01 | 7.431861e-01 | 5.006259e-01 | -4.482878e-01 | . max 2.828824e+00 | 2.907857e+00 | 7.431861e-01 | 7.256365e+00 | 7.111260e+00 | . . Note: Random forest algorithm does not need stardardized data and therefore we will not use this technique. . Three set of final features . Original features (raw_features) | Original features with minimum cleaning like feeling missing values (cleaned_original_features) | Original features with all created features (all_features) | The most useful features by reducing redundency (reduced_features) | . raw_features = [&#39;Pclass&#39;, &#39;Sex_numeric&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Cabin_numeric&#39;, &#39;Embarked_numeric&#39;] cleaned_original_features = [&#39;Pclass&#39;, &#39;Sex_numeric&#39;, &#39;Age_clean&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare_clean&#39;, &#39;Cabin_numeric&#39;, &#39;Embarked_clean_numeric&#39;] all_features = [&#39;Pclass&#39;, &#39;Sex_numeric&#39;, &#39;Age_clean&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;family_members&#39;, &#39;Fare_clean&#39;, &#39;Fare_clean_tr&#39;, &#39;Cabin_numeric&#39;, &#39;Cabin_info&#39;, &#39;Embarked_clean_numeric&#39;, &#39;Title&#39;, &#39;Embarked_numeric&#39;] reduced_features = [&#39;Pclass&#39;, &#39;Sex_numeric&#39;, &#39;Age_clean&#39;, &#39;family_members&#39;, &#39;Fare_clean_tr&#39;, &#39;Cabin_info&#39;, &#39;Ttile_clean&#39;] . Model comparision with different features . Process . Run the 5-fold cross-validation and select the best models for every feature sets discussed above. This is very important technique, used by many Kaggler. Kaggle grand master Abhishek thakur has written a wonderful book. I highly recommend to read it. | Evaluate those models on the validation set and pick the best model. | Evaluate the best model on the test set to guage its ability to generalize to unseen data. | Evaluation metrics . Precision and recall . from IPython.display import Image Image(url= &quot;https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg&quot;) . . Building mdoels . Original features . First, we will build model for raw features. The training dataset will look like below: . X_train[raw_features].head() . . Pclass Sex_numeric Age SibSp Parch Fare Cabin_numeric Embarked_numeric . 533 3 | 0 | -1000.0 | 0 | 2 | 22.3583 | 147 | 0 | . 51 3 | 1 | 21.0 | 0 | 0 | 7.8000 | 147 | 2 | . 823 3 | 0 | 27.0 | 0 | 1 | 12.4750 | 118 | 2 | . 709 3 | 1 | -1000.0 | 1 | 1 | 15.2458 | 147 | 0 | . 596 2 | 0 | -1000.0 | 0 | 0 | 33.0000 | 147 | 2 | . Below, you can see the correlation matrix for original features. We have already discussed this in the previous subsection. However, it is interesting to see correlation matrix for different sets of features. . matrix = np.triu(X_train[raw_features].corr()) plt.title(f&quot;Figure 7: Feature correlation matrix&quot;) sns.heatmap(X_train[raw_features].corr(), vmin=-1, vmax=1, annot=True, fmt=&#39;.1f&#39;, center=0, cmap=&#39;coolwarm&#39;, mask=matrix) plt.show() . . The performance of the Randomforest algorithm is highly dependent on hyperparameter tuning. Here, we will use Grid search method to find the best hyperparameter. . rf = RandomForestClassifier() parameters = { &#39;n_estimators&#39;: [ 2 ** i for i in range(1,9)], &#39;max_depth&#39;: [2,4,8, 15,17,20, None] } cv = GridSearchCV(rf, cv=5, param_grid=parameters) cv.fit(X_train[raw_features], y_train.values.ravel()) . . GridSearchCV(cv=5, estimator=RandomForestClassifier(), param_grid={&amp;#x27;max_depth&amp;#x27;: [2, 4, 8, 15, 17, 20, None], &amp;#x27;n_estimators&amp;#x27;: [2, 4, 8, 16, 32, 64, 128, 256]}) . In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=RandomForestClassifier(), param_grid={&amp;#x27;max_depth&amp;#x27;: [2, 4, 8, 15, 17, 20, None], &amp;#x27;n_estimators&amp;#x27;: [2, 4, 8, 16, 32, 64, 128, 256]}) . estimator: RandomForestClassifierRandomForestClassifier() . RandomForestClassifierRandomForestClassifier() . def print_result(results): print(f&quot;Best parameters: n{results.best_params_}&quot;) means = results.cv_results_[&#39;mean_test_score&#39;] stds = results.cv_results_[&#39;std_test_score&#39;] for mean, std, params in zip(means, stds, results.cv_results_[&#39;params&#39;]): print(f&quot;{round(mean,3)} (+/-{round(std*2,3)}) for {params} &quot;) print_result(cv) . Best parameters: {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 128} 0.751 (+/-0.083) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 2} 0.77 (+/-0.073) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 4} 0.764 (+/-0.085) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 8} 0.788 (+/-0.052) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 16} 0.772 (+/-0.057) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 32} 0.786 (+/-0.079) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 64} 0.772 (+/-0.048) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 128} 0.773 (+/-0.09) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 256} 0.773 (+/-0.116) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 2} 0.785 (+/-0.093) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 4} 0.786 (+/-0.071) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 8} 0.798 (+/-0.066) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 16} 0.807 (+/-0.07) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 32} 0.8 (+/-0.063) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 64} 0.811 (+/-0.073) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 128} 0.803 (+/-0.085) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 256} 0.745 (+/-0.101) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 2} 0.777 (+/-0.054) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 4} 0.798 (+/-0.043) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 8} 0.811 (+/-0.079) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 16} 0.826 (+/-0.066) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 32} 0.82 (+/-0.051) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 64} 0.835 (+/-0.072) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 128} 0.822 (+/-0.061) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 256} 0.762 (+/-0.05) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 2} 0.781 (+/-0.059) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 4} 0.803 (+/-0.067) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 8} 0.798 (+/-0.054) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 16} 0.792 (+/-0.084) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 32} 0.809 (+/-0.088) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 64} 0.809 (+/-0.067) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 128} 0.805 (+/-0.075) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 256} 0.77 (+/-0.048) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 2} 0.779 (+/-0.041) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 4} 0.796 (+/-0.058) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 8} 0.798 (+/-0.053) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 16} 0.811 (+/-0.06) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 32} 0.807 (+/-0.052) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 64} 0.803 (+/-0.08) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 128} 0.817 (+/-0.075) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 256} 0.76 (+/-0.068) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 2} 0.792 (+/-0.067) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 4} 0.802 (+/-0.04) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 8} 0.803 (+/-0.074) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 16} 0.802 (+/-0.054) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 32} 0.815 (+/-0.057) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 64} 0.805 (+/-0.08) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 128} 0.805 (+/-0.074) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 256} 0.77 (+/-0.091) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 2} 0.783 (+/-0.051) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 4} 0.789 (+/-0.077) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 8} 0.798 (+/-0.053) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 16} 0.822 (+/-0.067) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 32} 0.792 (+/-0.061) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 64} 0.803 (+/-0.073) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 128} 0.815 (+/-0.074) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 256} . . feat_imp = cv.best_estimator_.feature_importances_ indices = np.argsort(feat_imp) plt.yticks(range(len(indices)), [X_train[raw_features].columns[i] for i in indices]) plt.barh(range(len(indices)), feat_imp[indices], align=&#39;center&#39;) plt.title(f&quot;Figure 8: RandomForest feature importance &quot;) plt.show() . . Figure 8 shows the feature importance for the best randomforest model. Model gives the higherst importance to feature Sex and the lowest importance to feature Embarked which is plausible. We have also observed this while exploring the data. . Surprisingly, it give higher importance to Fare than Pclass. Fare is dependent on Pclass and cabin and not other way around. Here, model is getting confused. . Original features with minimum cleaning . matrix = np.triu(X_train[cleaned_original_features].corr()) plt.title(f&quot;Figure 9: Feature correlation matrix&quot;) sns.heatmap(X_train[cleaned_original_features].corr(), vmin=-1, vmax=1, annot=True, fmt=&#39;.1f&#39;, center=0, cmap=&#39;coolwarm&#39;, mask=matrix) plt.show() . . rf = RandomForestClassifier() parameters = { &#39;n_estimators&#39;: [ 2 ** i for i in range(1,9)], &#39;max_depth&#39;: [2,4,8, 15,17,20, None] } cv = GridSearchCV(rf, cv=5, param_grid=parameters) cv.fit(X_train[cleaned_original_features], y_train.values.ravel()) . . GridSearchCV(cv=5, estimator=RandomForestClassifier(), param_grid={&amp;#x27;max_depth&amp;#x27;: [2, 4, 8, 15, 17, 20, None], &amp;#x27;n_estimators&amp;#x27;: [2, 4, 8, 16, 32, 64, 128, 256]}) . In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=RandomForestClassifier(), param_grid={&amp;#x27;max_depth&amp;#x27;: [2, 4, 8, 15, 17, 20, None], &amp;#x27;n_estimators&amp;#x27;: [2, 4, 8, 16, 32, 64, 128, 256]}) . estimator: RandomForestClassifierRandomForestClassifier() . RandomForestClassifierRandomForestClassifier() . def print_result(results): print(f&quot;Best parameters: n{results.best_params_}&quot;) means = results.cv_results_[&#39;mean_test_score&#39;] stds = results.cv_results_[&#39;std_test_score&#39;] for mean, std, params in zip(means, stds, results.cv_results_[&#39;params&#39;]): print(f&quot;{round(mean,3)} (+/-{round(std*2,3)}) for {params} &quot;) print_result(cv) . Best parameters: {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 256} 0.762 (+/-0.073) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 2} 0.741 (+/-0.072) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 4} 0.755 (+/-0.104) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 8} 0.79 (+/-0.07) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 16} 0.773 (+/-0.042) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 32} 0.775 (+/-0.064) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 64} 0.788 (+/-0.071) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 128} 0.777 (+/-0.073) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 256} 0.779 (+/-0.097) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 2} 0.799 (+/-0.099) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 4} 0.798 (+/-0.073) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 8} 0.807 (+/-0.041) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 16} 0.824 (+/-0.074) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 32} 0.818 (+/-0.081) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 64} 0.83 (+/-0.083) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 128} 0.831 (+/-0.075) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 256} 0.74 (+/-0.044) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 2} 0.772 (+/-0.066) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 4} 0.811 (+/-0.049) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 8} 0.82 (+/-0.049) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 16} 0.833 (+/-0.076) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 32} 0.826 (+/-0.097) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 64} 0.837 (+/-0.063) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 128} 0.839 (+/-0.065) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 256} 0.757 (+/-0.047) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 2} 0.79 (+/-0.041) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 4} 0.804 (+/-0.087) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 8} 0.811 (+/-0.084) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 16} 0.809 (+/-0.05) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 32} 0.813 (+/-0.052) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 64} 0.826 (+/-0.08) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 128} 0.82 (+/-0.082) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 256} 0.768 (+/-0.037) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 2} 0.788 (+/-0.04) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 4} 0.796 (+/-0.057) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 8} 0.794 (+/-0.068) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 16} 0.818 (+/-0.066) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 32} 0.811 (+/-0.065) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 64} 0.807 (+/-0.083) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 128} 0.822 (+/-0.074) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 256} 0.77 (+/-0.047) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 2} 0.777 (+/-0.061) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 4} 0.794 (+/-0.045) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 8} 0.794 (+/-0.079) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 16} 0.809 (+/-0.075) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 32} 0.807 (+/-0.069) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 64} 0.824 (+/-0.11) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 128} 0.805 (+/-0.079) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 256} 0.736 (+/-0.055) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 2} 0.773 (+/-0.036) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 4} 0.803 (+/-0.075) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 8} 0.822 (+/-0.087) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 16} 0.807 (+/-0.049) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 32} 0.809 (+/-0.052) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 64} 0.815 (+/-0.083) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 128} 0.817 (+/-0.086) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 256} . . feat_imp = cv.best_estimator_.feature_importances_ indices = np.argsort(feat_imp) plt.yticks(range(len(indices)), [X_train[cleaned_original_features].columns[i] for i in indices]) plt.barh(range(len(indices)), feat_imp[indices], align=&#39;center&#39;) plt.title(f&quot;Figure 8: RandomForest feature importance &quot;) plt.show() . . All features . matrix = np.triu(X_train[all_features].corr()) plt.title(f&quot;Figure 9: Feature correlation matrix&quot;) sns.heatmap(X_train[all_features].corr(), vmin=-1, vmax=1, annot=True, fmt=&#39;.1f&#39;, center=0, cmap=&#39;coolwarm&#39;, mask=matrix) plt.show() . . rf = RandomForestClassifier() parameters = { &#39;n_estimators&#39;: [ 2 ** i for i in range(1,9)], &#39;max_depth&#39;: [2,4,8, 15,17,20, None] } cv = GridSearchCV(rf, cv=5, param_grid=parameters) cv.fit(X_train[all_features], y_train.values.ravel()) . . GridSearchCV(cv=5, estimator=RandomForestClassifier(), param_grid={&amp;#x27;max_depth&amp;#x27;: [2, 4, 8, 15, 17, 20, None], &amp;#x27;n_estimators&amp;#x27;: [2, 4, 8, 16, 32, 64, 128, 256]}) . In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=RandomForestClassifier(), param_grid={&amp;#x27;max_depth&amp;#x27;: [2, 4, 8, 15, 17, 20, None], &amp;#x27;n_estimators&amp;#x27;: [2, 4, 8, 16, 32, 64, 128, 256]}) . estimator: RandomForestClassifierRandomForestClassifier() . RandomForestClassifierRandomForestClassifier() . def print_result(results): print(f&quot;Best parameters: n{results.best_params_}&quot;) means = results.cv_results_[&#39;mean_test_score&#39;] stds = results.cv_results_[&#39;std_test_score&#39;] for mean, std, params in zip(means, stds, results.cv_results_[&#39;params&#39;]): print(f&quot;{round(mean,3)} (+/-{round(std*2,3)}) for {params} &quot;) print_result(cv) . Best parameters: {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 128} 0.738 (+/-0.113) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 2} 0.773 (+/-0.073) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 4} 0.794 (+/-0.066) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 8} 0.756 (+/-0.071) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 16} 0.786 (+/-0.062) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 32} 0.801 (+/-0.069) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 64} 0.794 (+/-0.073) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 128} 0.777 (+/-0.078) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 256} 0.766 (+/-0.092) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 2} 0.788 (+/-0.063) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 4} 0.818 (+/-0.065) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 8} 0.826 (+/-0.071) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 16} 0.811 (+/-0.073) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 32} 0.83 (+/-0.076) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 64} 0.833 (+/-0.078) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 128} 0.83 (+/-0.076) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 256} 0.755 (+/-0.078) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 2} 0.79 (+/-0.057) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 4} 0.824 (+/-0.075) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 8} 0.8 (+/-0.019) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 16} 0.818 (+/-0.064) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 32} 0.818 (+/-0.063) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 64} 0.83 (+/-0.068) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 128} 0.83 (+/-0.07) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 256} 0.743 (+/-0.051) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 2} 0.781 (+/-0.059) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 4} 0.788 (+/-0.078) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 8} 0.813 (+/-0.048) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 16} 0.809 (+/-0.083) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 32} 0.815 (+/-0.108) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 64} 0.807 (+/-0.072) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 128} 0.805 (+/-0.107) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 256} 0.758 (+/-0.047) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 2} 0.787 (+/-0.098) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 4} 0.79 (+/-0.069) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 8} 0.79 (+/-0.064) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 16} 0.798 (+/-0.078) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 32} 0.811 (+/-0.083) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 64} 0.802 (+/-0.1) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 128} 0.796 (+/-0.102) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 256} 0.76 (+/-0.078) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 2} 0.772 (+/-0.067) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 4} 0.811 (+/-0.088) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 8} 0.788 (+/-0.086) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 16} 0.809 (+/-0.077) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 32} 0.807 (+/-0.098) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 64} 0.809 (+/-0.092) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 128} 0.807 (+/-0.095) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 256} 0.729 (+/-0.085) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 2} 0.803 (+/-0.056) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 4} 0.788 (+/-0.053) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 8} 0.813 (+/-0.087) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 16} 0.809 (+/-0.088) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 32} 0.798 (+/-0.087) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 64} 0.8 (+/-0.094) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 128} 0.803 (+/-0.086) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 256} . . feat_imp = cv.best_estimator_.feature_importances_ indices = np.argsort(feat_imp) plt.yticks(range(len(indices)), [X_train[all_features].columns[i] for i in indices]) plt.barh(range(len(indices)), feat_imp[indices], align=&#39;center&#39;) plt.title(f&quot;Figure 10: RandomForest feature importance &quot;) plt.show() . . Reduced features . matrix = np.triu(X_train[reduced_features].corr()) plt.title(f&quot;Figure 11: Feature correlation matrix&quot;) sns.heatmap(X_train[reduced_features].corr(), vmin=-1, vmax=1, annot=True, fmt=&#39;.1f&#39;, center=0, cmap=&#39;coolwarm&#39;, mask=matrix) plt.show() . . rf = RandomForestClassifier() parameters = { &#39;n_estimators&#39;: [ 2 ** i for i in range(1,9)], &#39;max_depth&#39;: [2,4,8, 15,17,20, None] } cv = GridSearchCV(rf, cv=5, param_grid=parameters) cv.fit(X_train[reduced_features], y_train.values.ravel()) . . GridSearchCV(cv=5, estimator=RandomForestClassifier(), param_grid={&amp;#x27;max_depth&amp;#x27;: [2, 4, 8, 15, 17, 20, None], &amp;#x27;n_estimators&amp;#x27;: [2, 4, 8, 16, 32, 64, 128, 256]}) . In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=RandomForestClassifier(), param_grid={&amp;#x27;max_depth&amp;#x27;: [2, 4, 8, 15, 17, 20, None], &amp;#x27;n_estimators&amp;#x27;: [2, 4, 8, 16, 32, 64, 128, 256]}) . estimator: RandomForestClassifierRandomForestClassifier() . RandomForestClassifierRandomForestClassifier() . def print_result(results): print(f&quot;Best parameters: n{results.best_params_}&quot;) means = results.cv_results_[&#39;mean_test_score&#39;] stds = results.cv_results_[&#39;std_test_score&#39;] for mean, std, params in zip(means, stds, results.cv_results_[&#39;params&#39;]): print(f&quot;{round(mean,3)} (+/-{round(std*2,3)}) for {params} &quot;) print_result(cv) . Best parameters: {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 32} 0.783 (+/-0.075) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 2} 0.785 (+/-0.063) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 4} 0.796 (+/-0.055) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 8} 0.8 (+/-0.065) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 16} 0.807 (+/-0.091) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 32} 0.8 (+/-0.073) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 64} 0.79 (+/-0.073) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 128} 0.796 (+/-0.05) for {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 256} 0.798 (+/-0.099) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 2} 0.801 (+/-0.077) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 4} 0.83 (+/-0.054) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 8} 0.824 (+/-0.056) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 16} 0.837 (+/-0.098) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 32} 0.826 (+/-0.081) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 64} 0.83 (+/-0.091) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 128} 0.832 (+/-0.075) for {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 256} 0.777 (+/-0.077) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 2} 0.794 (+/-0.077) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 4} 0.831 (+/-0.057) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 8} 0.828 (+/-0.079) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 16} 0.824 (+/-0.084) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 32} 0.83 (+/-0.084) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 64} 0.822 (+/-0.075) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 128} 0.828 (+/-0.083) for {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 256} 0.77 (+/-0.072) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 2} 0.785 (+/-0.068) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 4} 0.811 (+/-0.086) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 8} 0.818 (+/-0.08) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 16} 0.818 (+/-0.073) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 32} 0.809 (+/-0.093) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 64} 0.811 (+/-0.09) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 128} 0.813 (+/-0.085) for {&#39;max_depth&#39;: 15, &#39;n_estimators&#39;: 256} 0.753 (+/-0.038) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 2} 0.801 (+/-0.062) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 4} 0.809 (+/-0.049) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 8} 0.809 (+/-0.075) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 16} 0.805 (+/-0.078) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 32} 0.802 (+/-0.074) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 64} 0.805 (+/-0.094) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 128} 0.811 (+/-0.084) for {&#39;max_depth&#39;: 17, &#39;n_estimators&#39;: 256} 0.762 (+/-0.101) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 2} 0.794 (+/-0.068) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 4} 0.796 (+/-0.1) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 8} 0.796 (+/-0.085) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 16} 0.82 (+/-0.073) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 32} 0.811 (+/-0.09) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 64} 0.813 (+/-0.08) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 128} 0.805 (+/-0.088) for {&#39;max_depth&#39;: 20, &#39;n_estimators&#39;: 256} 0.777 (+/-0.066) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 2} 0.785 (+/-0.057) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 4} 0.809 (+/-0.077) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 8} 0.815 (+/-0.08) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 16} 0.809 (+/-0.093) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 32} 0.815 (+/-0.09) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 64} 0.807 (+/-0.081) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 128} 0.807 (+/-0.077) for {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 256} . . feat_imp = cv.best_estimator_.feature_importances_ indices = np.argsort(feat_imp) plt.yticks(range(len(indices)), [X_train[reduced_features].columns[i] for i in indices]) plt.barh(range(len(indices)), feat_imp[indices], align=&#39;center&#39;) plt.title(f&quot;Figure 12: RandomForest feature importance &quot;) plt.show() . . Model comparison . now, we will select the best model based on validation dataset which was not exposed during training. . ======================================== Model comparision for validation dataset ======================================== Raw features Time taken: 0.0212 max_depth: 8, n_estimator: 128 Accuracy: 0.79, Recall: 0.57, precision: 0.81, f1: 0.67 Raw features with minimum cleaning Time taken: 0.0323 max_depth: 8, n_estimator: 256 Accuracy: 0.78, Recall: 0.6, precision: 0.77, f1: 0.68 All features Time taken: 0.0128 max_depth: 4, n_estimator: 128 Accuracy: 0.79, Recall: 0.59, precision: 0.8, f1: 0.68 Reduced features Time taken: 0.0044 max_depth: 4, n_estimator: 32 Accuracy: 0.81, Recall: 0.65, precision: 0.81, f1: 0.72 . =================================== Model comparision for test dataset =================================== Raw features Time taken: 0.07 max_depth: 8, n_estimator:128 Accuracy: 0.84, Recall: 0.74, precision: 0.82, f1: 0.78 Raw features with minimum cleaning Time taken: 0.0682 max_depth: 8, n_estimator:256 Accuracy: 0.84, Recall: 0.74, precision: 0.84, f1: 0.78 All features Time taken: 0.0484 max_depth: 4, n_estimator:128 Accuracy: 0.86, Recall: 0.78, precision: 0.84, f1: 0.81 Reduced features Time taken: 0.0064 max_depth: 4, n_estimator:32 Accuracy: 0.85, Recall: 0.75, precision: 0.85, f1: 0.8 . The model comarison for validation dataset reveals that the model trained with reduced features has the highest f1 score and also takes lesser time to predict the target. It clearly dominates other models. . However, for test dataset the model trained with all features provides better result as compare to other models but also takes more time to predict. In this case, I will select the model trained with reduced features as the model performance is very close to the model trained with all features and takes less time to predict. . &lt;/div&gt; .",
            "url": "https://pandya6988.github.io/my_first_fastpage/fastpages/feature%20engineering/machine%20learning/2022/09/11/feature_engineering.html",
            "relUrl": "/fastpages/feature%20engineering/machine%20learning/2022/09/11/feature_engineering.html",
            "date": " • Sep 11, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://pandya6988.github.io/my_first_fastpage/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://pandya6988.github.io/my_first_fastpage/jupyter/2020/02/20/test-copy.html",
            "relUrl": "/jupyter/2020/02/20/test-copy.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://pandya6988.github.io/my_first_fastpage/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://pandya6988.github.io/my_first_fastpage/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pandya6988.github.io/my_first_fastpage/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}